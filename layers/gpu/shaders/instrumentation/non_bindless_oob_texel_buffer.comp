// Copyright (c) 2024 The Khronos Group Inc.
// Copyright (c) 2024 Valve Corporation
// Copyright (c) 2024 LunarG, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// NOTE: This file doesn't contain any entrypoints and should be compiled with then new --no-link option for glslang

#version 450
#extension GL_GOOGLE_include_directive : enable
#extension GL_EXT_buffer_reference : require
#extension GL_EXT_buffer_reference_uvec2 : require
#if defined(GL_ARB_gpu_shader_int64)
#extension GL_ARB_gpu_shader_int64 : require
#else
#error No extension available for 64-bit integers.
#endif

#include "gpu_error_header.h"
#include "gpu_shaders_constants.h"
#include "common_descriptor_sets.h"

layout(buffer_reference, buffer_reference_align = 8, std430) buffer DescriptorLayoutData {
    // BindingLayout[0]
    uint num_bindings;
    uint pad; // always zero, used to keep things aligned

    // BindingLayout[1] - BindingLayout[N]
    // struct glsl::BindingLayout {
    //   x: count
    //   y: state_start
    // }
    uvec2 data[];
};

layout(buffer_reference, buffer_reference_align = 8, std430) buffer DescriptorSetInData {
    // struct glsl::DescriptorState {
    //   x: id
    //   y: extra data depending on the descriptor type
    // }
    uvec2 data[];
};

layout(buffer_reference, buffer_reference_align = 8, std430) buffer GlobalState {
    // Maps to DescriptorHeap and used to detect if descriptor is still valid on CPU
    uint data[];
};

struct DescriptorSetRecord {
    DescriptorLayoutData layout_data;
    DescriptorSetInData in_data;
    uvec2 out_data; // unused BDA pointer
};

layout(set = kInstDefaultDescriptorSet, binding = kBindingInstBindlessDescriptor, std430) buffer BindlessStateBuffer {
    GlobalState global_state;
    DescriptorSetRecord desc_sets[kDebugInputBindlessMaxDescSets];
} bindless_state_buffer;

// TODO - This currently the almost exact same code as inst_non_bindless_oob_buffer
// We do not want to waste time deciding which descriptor type it is inside the shader.
// While this is some code duplication, it keeps things seperated for non-bindlesss everywhere else
bool inst_non_bindless_oob_texel_buffer(const uint inst_num, const uvec4 stage_info, const uint desc_array_size,
                        const uint desc_set, const uint binding, const uint desc_index, const uint byte_offset) {
    uint error = 0u;
    uint param_0 = 0u;
    uint param_1 = 0u;
    do {
        // For non-array this should hopefully optimized out as "if (0 > 1)"
        if (desc_index > desc_array_size) {
            error = kErrorSubCodeNonBindlessOOBTexelBufferArrays;
            param_0 = desc_array_size;
            break;
        }

        DescriptorLayoutData layout_data = bindless_state_buffer.desc_sets[desc_set].layout_data;
        uvec2 binding_state = layout_data.data[binding];

        DescriptorSetInData in_data = bindless_state_buffer.desc_sets[desc_set].in_data;

        uint state_index = binding_state.y + desc_index;

        // check that the offset is in bounds
        uint resource_size = in_data.data[state_index].y;
        if (byte_offset >= resource_size) {
            error = kErrorSubCodeNonBindlessOOBTexelBufferBounds;
            param_0 = byte_offset;
            param_1 = resource_size;
            break;
        }
    } while (false);

    if (0u != error) {

        const uint cmd_id = inst_cmd_resource_index_buffer.index[0];
        const uint cmd_errors_count = atomicAdd(inst_cmd_errors_count_buffer.errors_count[cmd_id], 1);
        const bool max_cmd_errors_count_reached = cmd_errors_count >= kMaxErrorsPerCmd;

        if (max_cmd_errors_count_reached) return false;

        uint write_pos = atomicAdd(inst_errors_buffer.written_count, kErrorRecordSize);
        const bool errors_buffer_not_filled = (write_pos + kErrorRecordSize) <= uint(inst_errors_buffer.data.length());

        if (errors_buffer_not_filled) {
            inst_errors_buffer.data[write_pos + kHeaderErrorRecordSizeOffset] = kErrorRecordSize;
            inst_errors_buffer.data[write_pos + kHeaderShaderIdOffset] = kLinkShaderId;
            inst_errors_buffer.data[write_pos + kHeaderInstructionIdOffset] = inst_num;
            inst_errors_buffer.data[write_pos + kHeaderStageIdOffset] = stage_info.x;
            inst_errors_buffer.data[write_pos + kHeaderStageInfoOffset_0] = stage_info.y;
            inst_errors_buffer.data[write_pos + kHeaderStageInfoOffset_1] = stage_info.z;
            inst_errors_buffer.data[write_pos + kHeaderStageInfoOffset_2] = stage_info.w;

            inst_errors_buffer.data[write_pos + kHeaderErrorGroupOffset] = kErrorGroupInstNonBindlessOOB;
            inst_errors_buffer.data[write_pos + kHeaderErrorSubCodeOffset] = error;

			inst_errors_buffer.data[write_pos + kHeaderActionIdOffset] = inst_action_index_buffer.index[0];
            inst_errors_buffer.data[write_pos + kHeaderCommandResourceIdOffset] = inst_cmd_resource_index_buffer.index[0];

            inst_errors_buffer.data[write_pos + kInstBindlessDescSetOffset] = desc_set;
            inst_errors_buffer.data[write_pos + kInstBindlessDescBindingOffset] = binding;
            inst_errors_buffer.data[write_pos + kInstBindlessDescIndexOffset] = desc_index;
            inst_errors_buffer.data[write_pos + kInstBindlessCustomOffset_0] = param_0;
            inst_errors_buffer.data[write_pos + kInstBindlessCustomOffset_1] = param_1;
        }
        return false;
    }
    return true;
}